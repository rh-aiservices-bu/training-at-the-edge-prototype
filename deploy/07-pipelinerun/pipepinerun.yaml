---
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  # name: training-pipeline
  generateName: training-pipeline-
  annotations:
    tekton.dev/output_artifacts: '{"train-model": [{"key": "artifacts/$PIPELINERUN/train-model/mlpipeline-metrics.tgz",
      "name": "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/train-model/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/tmp/mlpipeline-ui-metadata.json"}], "upload-model": [{"key": "artifacts/$PIPELINERUN/upload-model/mlpipeline-metrics.tgz",
      "name": "mlpipeline-metrics", "path": "/tmp/mlpipeline-metrics.json"}, {"key":
      "artifacts/$PIPELINERUN/upload-model/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/tmp/mlpipeline-ui-metadata.json"}]}'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"train-model": [["mlpipeline-metrics", "/tmp/mlpipeline-metrics.json"],
      ["mlpipeline-ui-metadata", "/tmp/mlpipeline-ui-metadata.json"]], "upload-model":
      [["mlpipeline-metrics", "/tmp/mlpipeline-metrics.json"], ["mlpipeline-ui-metadata",
      "/tmp/mlpipeline-ui-metadata.json"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "train at the edge", "name":
      "train"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  pipelineSpec:
    tasks:
    - name: upload-artifacts
      taskSpec:
        results:
        - name: datetime
          type: string
        steps:
        - name: main
          args:
            - -ec
            - |
              oc get secret minio-root-user
              env | grep MINIO
              cd /tmp
              echo  "do the wget"
              pwd
              ls -al
              rm -rf *.tar.gz
              BRANCH="main"
              # BRANCH="hooking-things-together"
              # fallocate -l 0.5G train-step.tar.gz
              wget --no-check-certificate -q https://github.com/rh-aiservices-bu/training-at-the-edge-prototype/raw/${BRANCH}/training-code/pipeline-artifacts/training-artifacts/train-step.tar.gz -O train-step.tar.gz
              wget --no-check-certificate -q https://github.com/rh-aiservices-bu/training-at-the-edge-prototype/raw/${BRANCH}/training-code/pipeline-artifacts/training-artifacts/upload-step.tar.gz -O upload-step.tar.gz
              # pip list
              echo  "try to upload"
              cat << 'EOF' | python3

              import boto3, os
              from boto3.s3.transfer import TransferConfig
              import botocore
              import time

              print("Uploading...")

              current_time = int(time.time())

              f = open(".time", "w")
              f.write(f"{current_time}")
              f.close()

              bucket_name = 'pipeline-artifacts'
              endpoint_url="http://minio:9000"
              aws_access_key_id=os.getenv("MINIO_ROOT_USER")
              aws_secret_access_key=os.getenv("MINIO_ROOT_PASSWORD")

              s3 = boto3.client("s3",
                            endpoint_url=endpoint_url,
                            aws_access_key_id=aws_access_key_id,
                            aws_secret_access_key=aws_secret_access_key)

              GB = 1024 ** 3
              config = TransferConfig(multipart_threshold=5*GB, use_threads=False)
              s3.upload_file("train-step.tar.gz", bucket_name, f"edge/training-artifacts-{current_time}/train-step.tar.gz", Config=config)
              s3.upload_file("upload-step.tar.gz", bucket_name, f"edge/training-artifacts-{current_time}/upload-step.tar.gz")

              EOF
              echo "upload done"
              echo $(cat .time) | tee $(results.datetime.path)
          command:
            - sh
            - -c
          image: quay.io/rlundber/sds-small:1.8
          envFrom:
            - secretRef:
                name: minio-root-user
    - name: train-model
      params:
        - name: datetime
          value: $(tasks.upload-artifacts.results.datetime)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            export DATETIME=$(params.datetime)
            echo $DATETIME
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'train' --cos-endpoint 'http://minio:9000' --cos-bucket 'pipeline-artifacts' --cos-directory 'edge/training-artifacts-$DATETIME' --cos-dependencies-archive 'train-step.tar.gz' --file 'training-at-the-edge-prototype/training-code/train.py' --outputs 'model.onnx' "
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-pipeline-artifacts
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-pipeline-artifacts
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-my-storage
          - name: AWS_S3_BUCKET
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-my-storage
          - name: AWS_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-my-storage
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-my-storage
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                key: AWS_DEFAULT_REGION
                name: aws-connection-my-storage
          image: quay.io/rlundber/sds-small:1.8
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: train
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: train
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: training-at-the-edge-prototype/training-code/train.py
            elyra/pipeline-source: train.pipeline
            pipelines.kubeflow.org/task_display_name: train
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=97249c45e81a65d0b17c1e6975d0fd438ba3624f7a464891ea373c416147f998"}'
      runAfter:
        - upload-artifacts
    - name: upload-model
      params:
        - name: datetime
          value: $(tasks.upload-artifacts.results.datetime)
      taskSpec:
        results:
        - name: modelversion
          type: string
        steps:
        - name: main
          args:
          - |
            export DATETIME=$(params.datetime)
            echo $DATETIME
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'train' --cos-endpoint 'http://minio:9000' --cos-bucket 'pipeline-artifacts' --cos-directory 'edge/training-artifacts-$DATETIME' --cos-dependencies-archive 'upload-step.tar.gz' --file 'training-at-the-edge-prototype/training-code/upload.py' --inputs 'model.onnx' --outputs '.model-version' "
            sh -c "echo This is the model version: $(cat .model-version)"
            sh -c "echo $(cat .model-version) | tee $(results.modelversion.path)"
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-pipeline-artifacts
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-pipeline-artifacts
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-my-storage
          - name: AWS_S3_BUCKET
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-my-storage
          - name: AWS_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-my-storage
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-my-storage
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                key: AWS_DEFAULT_REGION
                name: aws-connection-my-storage
          image: quay.io/rlundber/sds-small:1.8
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: train
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: upload
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: training-at-the-edge-prototype/training-code/upload.py
            elyra/pipeline-source: train.pipeline
            pipelines.kubeflow.org/task_display_name: upload
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=5cbd16430aa20d95bc9f685859563ea4c04a687e71b82769839d2cbd99fef667"}'
      runAfter:
      - train-model
    - name: update-served-model
      params:
        - name: SCRIPT
          value: |-
            export MODEL_VERSION=$(tasks.upload-model.results.modelversion)
            echo Model Version $MODEL_VERSION
            oc apply -f - <<EOF
            apiVersion: serving.kserve.io/v1beta1
            kind: InferenceService
            metadata:
              name: fraud-latest
              annotations:
                serving.kserve.io/deploymentMode: ModelMesh
                openshift.io/display-name: Fraud Detection (latest)
              namespace: edgetraining
              labels:
                name: fraud-latest
                opendatahub.io/dashboard: "true"
            spec:
              predictor:
                model:
                  modelFormat:
                    name: onnx
                    version: '1'
                  runtime: ovms-server-01
                  storage:
                    key: aws-connection-my-storage
                    path: models/$MODEL_VERSION/model.onnx
            ---
            apiVersion: serving.kserve.io/v1beta1
            kind: InferenceService
            metadata:
              name: fraud-$MODEL_VERSION
              annotations:
                serving.kserve.io/deploymentMode: ModelMesh
                openshift.io/display-name: Fraud Detection (v$MODEL_VERSION)
              namespace: edgetraining
              labels:
                name: fraud-$MODEL_VERSION
                opendatahub.io/dashboard: "true"
            spec:
              predictor:
                model:
                  modelFormat:
                    name: onnx
                    version: '1'
                  runtime: ovms-server-01
                  storage:
                    key: aws-connection-my-storage
                    path: models/$MODEL_VERSION/model.onnx
            EOF
        - name: VERSION
          value: '4.7'
      taskRef:
        kind: Task
        name: openshift-client
      runAfter:
      - upload-model
